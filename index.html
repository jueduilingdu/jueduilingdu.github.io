<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Mengshi Qi</title>
  
  <meta name="author" content="Mengshi Qi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/buaa.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mengshi Qi</name>
              </p>
              <p>I am currently a postdoctoral researcher in <a href="https://www.epfl.ch/labs/cvlab/">CVLab</a> at <a href="https://www.epfl.ch/en/">École polytechnique fédérale de Lausanne</a> (EPFL), where I work closely with <a href="https://people.epfl.ch/pascal.fua">Prof. Pascal Fua</a> and <a href="https://people.epfl.ch/mathieu.salzmann">Dr. Mathieu Salzmann</a>.
              </p>
              <p>
                 Prior to that, I have ever worked at <a href="http://research.baidu.com/">Baidu Research</a>, where I focus on computer vision and deep learning collaborated with <a href="https://www.uts.edu.au/staff/yi.yang">Prof. Yi Yang</a> in 2019. I did my PhD and Master at <a href="https://www.buaa.edu.cn/">Beihang University</a> (BUAA) in 2019 and 2014, respectively, where I was advised by <a href="http://irip.buaa.edu.cn/yhwang/index.html">Prof. Yunhong Wang</a>. Especially, I had been a visting PhD at <a href="https://www.rochester.edu/">University of Rochester</a> supervised by <a href="https://www.cs.rochester.edu/u/jluo/">Prof. Jiebo Luo</a> from 2017 to 2018. I did my bachelors at <a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications</a> (BUPT) in 2012.
              </p>
              <p style="text-align:center">
                <a href="mailto:qidash@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/mengshiqi_cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=_gH7-4wAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/mengshi-qi-684abb97/"> LinkedIn </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/qimengshi_circle.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/qimengshi_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Interests</heading>
              <p>
                I'm interested in computer vision and machine learning, especially scene understanding, 3D reconstruction and multimedia analysis. Most of my research is about how to understand the semantic content and infer the physical information from images and videos.
              </p>
            </td>
          </tr>
        </tbody></table>
		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
		
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
		 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2020imi.jpg' width="180" height="160">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_Imitative_Non-Autoregressive_Modeling_for_Trajectory_Forecasting_and_Imputation_CVPR_2020_paper.pdf">
                <papertitle>Imitative Non-Autoregressive Modeling for Trajectory Forecasting and Imputation</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
              <a href="https://sites.google.com/site/firmamentqj/">Jie Qin</a>,
              <a href="https://yu-wu.net/">Yu Wu</a>,
			  <a href="https://www.uts.edu.au/staff/yi.yang">Yi Yang</a>
              <br>
              <em>CVPR</em>, 2020
              <br>
              <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_Imitative_Non-Autoregressive_Modeling_for_Trajectory_Forecasting_and_Imputation_CVPR_2020_paper.pdf">pdf</a> /
			  <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_Imitative_Non-Autoregressive_Modeling_for_Trajectory_Forecasting_and_Imputation_CVPR_2020_paper.pdf">press</a> /
              <a href="data/qi2020imi.bib">bibtex</a>
              <p></p>
              <p> In this paper, we proposed a novel Imitative Non-Autoregressive Modeling method to bridge the performance gap between autoregressive and non-autoregressive models for temporal sequence forecasting and imputation. Our proposed framework leveraged an imitation learning fashion including two parts, i.e., a recurrent conditional variational autoencoder (RC-VAE) demonstrator and a nonautoregressive transformation model (NART) learner.</p>
            </td>
          </tr>

		 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2020fewshot.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Few-Shot Ensemble Learning for Video Classificaion with SlowFast Memory Networks</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
			  <a href="https://sites.google.com/site/firmamentqj/">Jie Qin</a>,
              <a href="https://scholar.google.ca/citations?user=DnBb3e0AAAAJ&hl=en">Xiantong Zhen</a>,
              <a href="http://irip.buaa.edu.cn/dihuang/index.html">Di Huang</a>,
			  <a href="https://www.uts.edu.au/staff/yi.yang">Yi Yang</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>ACM MM</em>, 2020
              <br>
			  <a href="">pdf</a> /
			  <a href="">press</a> /
              <a href="data/qi2020fewshot.bib">bibtex</a>
              <p></p>
              <p>In this paper, we address few-shot video classification by learning an ensemble of SlowFast networks augmented with memory units. Specifically, we introduce a family of few-shot learners based on SlowFast networks which are used to extract informative features at multiple rates, and we incorporate a memory unit into each network to enable encoding and retrieving crucial information instantly.</p>
            </td>
          </tr>		  
		
		 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2020stc.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2020stcgan.pdf">
                <papertitle>STC-GAN: Spatio-Temporally Coupled Generative Adversarial Networks for Predictive Scene Parsing</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>IEEE Transactions on Image Processing (TIP)</em>, 2020
              <br>
			  <a href="data/qi2020stcgan.pdf">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/document/9052709">press</a> /
              <a href="data/qi2020stc.bib">bibtex</a>
              <p></p>
              <p>In this paper, we present a novel Generative Adversarial Networks-based model (i.e., STC-GAN) for predictive scene parsing. STC-GAN captures both spatial and temporal representations from the observed frames of a video through CNN and convolutional LSTM network. Moreover, a coupled architecture is employed to guide the adversarial training via a weight-sharing mechanism and a feature adaptation transform between the future frame generation model and the predictive scene parsing model.</p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2019sports.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function nightsight_start() {
                  document.getElementById('nightsight_image').style.opacity = "1";
                }

                function nightsight_stop() {
                  document.getElementById('nightsight_image').style.opacity = "0";
                }
                nightsight_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2019sports.pdf">
                <papertitle>Sports Video Captioning via Attentive Motion Representation and Group Relationship Modeling</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi</strong>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>IEEE Transactions on Circuits and Systems fo Video Technology (TCSVT)</em>, 2019 &nbsp <font color="red"><strong>(An extension of our MMSports@MM 18' paper)</strong></font>
              <br>
              <a href="data/qi2019sports.pdf">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/abstract/document/8733019">press</a> /
              <a href="data/qi2019sports.bib">bibtex</a>
              <p></p>
              <p>  In this study, we present a novel hierarchical recurrent neural network based framework with an attention mechanism for sports video captioning, in which a motion representation module is proposed to capture individual pose attribute and dynamical trajectory cluster information with extra professional sports knowledge, and a group relationship module is employed to design a scene graph for modeling players’ interaction by a gated graph convolutional network.</p>
            </td>
          </tr>
          
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2019stagnet.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2019stagnet.pdf">
                <papertitle>stagNet: An Attentive Semantic RNN for Group Activity and Individual Action Recognition</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="https://sites.google.com/site/firmamentqj/">Jie Qin</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>,
			  <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>,
			  <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a>
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2019 &nbsp <font color="red"><strong>(An extension of our ECCV 18' paper)</strong></font>
              <br>
              <a href="data/qi2019stagnet.pdf">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/abstract/document/8621027">press</a> /
              <a href="data/qi2019stagnet.bib">bibtex</a>
              <p></p>
              <p> In the paper, we present a novel attentive semantic recurrent neural network (RNN), namely stagNet, for understanding group activities and individual actions in videos, by combining the spatio-temporal attention mechanism and semantic graph modeling. Specifically, a structured semantic graph is explicitly modeled to express the spatial contextual content of the whole scene, which is afterward further incorporated with the temporal factor through structural- RNN. </p>
            </td>
          </tr>
          
          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2019attentive.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function dpzlearn_start() {
                  document.getElementById('dpzlearn_image').style.opacity = "1";
                }

                function dpzlearn_stop() {
                  document.getElementById('dpzlearn_image').style.opacity = "0";
                }
                dpzlearn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qi_Attentive_Relational_Networks_for_Mapping_Images_to_Scene_Graphs_CVPR_2019_paper.pdf">
                <papertitle>Attentive Relational Networks for Mapping Images to Scene Graphs</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi*</strong>,
              <a href="https://www.cs.rochester.edu/u/wli69/">Weijian Li*</a>,
              <a href="https://www.cs.rochester.edu/u/zyang39/">Zhengyuan Yang</a>,
			  <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
			  <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>CVPR</em>, 2019
              <br>
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qi_Attentive_Relational_Networks_for_Mapping_Images_to_Scene_Graphs_CVPR_2019_paper.pdf">pdf</a> /
			  <a href="https://arxiv.org/abs/1811.10696">arxiv</a> /
              <a href="data/qi2019attentive.bib">bibtex</a>
              <p></p>
              <p> In this study, we propose a novel Attentive Relational Network that consists of two key modules with an object detection backbone to approach this problem. The first module is a semantic transformation module utilized to capture semantic embedded relation features, by translating visual features and linguistic features into a common semantic space. The other module is a graph self-attention module introduced to embed a joint graph representation through assigning various importance weights to neighboring nodes. </p>
            </td>
          </tr>
          
          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2019ke.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function porlight_start() {
                  document.getElementById('porlight_image').style.opacity = "1";
                }

                function porlight_stop() {
                  document.getElementById('porlight_image').style.opacity = "0";
                }
                porlight_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qi_KE-GAN_Knowledge_Embedded_Generative_Adversarial_Networks_for_Semi-Supervised_Scene_Parsing_CVPR_2019_paper.pdf">
                <papertitle>KE-GAN: Knowledge Embedded Generative Adversarial Networks for Semi-Supervised Scene Parsing</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="https://sites.google.com/site/firmamentqj/">Jie Qin</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>
              <br>
              <em>CVPR</em>, 2019
              <br>
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qi_KE-GAN_Knowledge_Embedded_Generative_Adversarial_Networks_for_Semi-Supervised_Scene_Parsing_CVPR_2019_paper.pdf">pdf</a> /
              <a href="data/qi2019ke.bib">bibtex</a>
              <br>
              <p></p>
              <p>In this paper, we propose a novel Knowledge Embedded Generative Adversarial Networks, dubbed as KE-GAN, to tackle the challenging problem in a semi-supervised fashion. KE-GAN captures semantic consistencies of different categories by devising a Knowledge Graph from the large-scale text corpus.</p>
            </td>
          </tr>

          <tr onmouseout="mpi_stop()" onmouseover="mpi_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2018stagnet.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }

                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Mengshi_Qi_stagNet_An_Attentive_ECCV_2018_paper.pdf">
                <papertitle>stagNet: An Attentive Semantic RNN for Group Activity Recognition</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi</strong>,
              <a href="https://sites.google.com/site/firmamentqj/">Jie Qin</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
			  <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>,
			  <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a>
              <br>
              <em>ECCV</em>, 2018
              <br>
              <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Mengshi_Qi_stagNet_An_Attentive_ECCV_2018_paper.pdf">pdf</a> /
              <a href="data/qi2018stagnet.bib">bibtex</a>
              <p></p>
              <p> We propose a novel attentive semantic recurrent neural network (RNN), dubbed as stagNet, for understanding group activities in videos, based on the spatio-temporal attention and semantic graph.  </p>
            </td>
          </tr>

          <tr onmouseout="mpi_stop()" onmouseover="mpi_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2018sports.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function mpi_start() {
                  document.getElementById('mpi_image').style.opacity = "1";
                }

                function mpi_stop() {
                  document.getElementById('mpi_image').style.opacity = "0";
                }
                mpi_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2018sports.pdf">
                <papertitle>Sports Video Captioning by Attentive Motion Representation based Hierarchical Recurrent Neural Networks</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi</strong>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>MMSports@MM</em>, 2018 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="data/qi2018sports.pdf">pdf</a> /
              <a href="https://www.youtube.com/watch?v=AUHGMF-kvDc">video</a> /
			  <a href="https://dl.acm.org/citation.cfm?id=3265851">press</a> /			  
              <a href="data/qi2018sports.bib">bibtex</a>
              <p></p>
              <p> In this paper, we present a novel hierarchical recurrent neural network (RNN) based framework with an attention mechanism for sports video captioning. A motion representation module is proposed to extract individual pose attribute and group-level trajectory cluster information. </p>
            </td>
          </tr>

          <tr onmouseout="unprocessing_stop()" onmouseover="unprocessing_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2017online.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function unprocessing_start() {
                  document.getElementById('unprocessing_image').style.opacity = "1";
                }

                function unprocessing_stop() {
                  document.getElementById('unprocessing_image').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2017online.pdf">
                <papertitle>Online Cross-modal Scene Retrieval by Binary Representation and Semantic Graph</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>
              <br>
              <em>ACM MM</em>, 2017
              <br>
              <a href="data/qi2017online.pdf">pdf</a> /
              <a href="https://dl.acm.org/citation.cfm?id=3123311">press</a> /
              <a href="data/qi2017online.bib">bibtex</a>
              <p></p>
              <p>We propose a new framework for online cross-modal scene retrieval based on binary representations and semantic graph. Specially, we adopt the cross-modal hashing based on the quantization loss of different modalities. By introducing the semantic graph, we are able to extract wealthy semantics and measure their correlation across different modalities. </p>
            </td>
          </tr>

          <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2016deep.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2016deep.pdf">
                <papertitle>DEEP-CSSR: Scene Classification using Category-specific Salient Region with Deep Features</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi</strong>,
			  <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>
              <br>
              <em>IEEE ICIP</em>, 2016 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="data/qi2016deep.pdf">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/abstract/document/7532517">press</a> /
              <a href="data/qi2016deep.bib">bibtex</a>
              <p></p>
              <p>In this paper, we introduce a novel framework towards scene classification using category-specific salient region(CSSR) with deep CNN features, called Deep-CSSR. </p>
            </td>
          </tr>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              Conference Reviewer, ICLR 2021/NeurIPS 2020/CVPR 2020/ECCV 2020/ICCV 2019/MM 2019
              <br>
			  <br>
              Journal Reviewer, TIP/TCSVT/PR/ACM Computing Surveys
			  <br>
              <br>
              PC Member, AAAI 2020
			  <br>
              <br>
              IEEE Member and ACM Member
            </td>
          </tr>
        </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <br>
              <br>
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks to this <a href="https://jonbarron.info/">awesome guy</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
		
		
      </td>
    </tr>
  </table>
</body>

</html>
