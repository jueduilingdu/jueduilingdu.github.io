<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Mengshi Qi</title>
  
  <meta name="author" content="Mengshi Qi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/bupt.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mengshi Qi</name>
              </p>
			   <p>I am currently a Professor in School of Computer Science at <a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications</a> (BUPT) where I lead the Multimedia Intelligent Computing Lab (<a href="https://github.com/MICLAB-BUPT/">MICLAB@BUPT</a>).   
              </p>
              <p>
                 Prior to that, I have ever worked as a post-doc research scientist in <a href="https://www.epfl.ch/labs/cvlab/">CVLab</a> at <a href="https://www.epfl.ch/en/">École polytechnique fédérale de Lausanne</a> (EPFL) from 2019 to 2021, working closely with <a href="https://people.epfl.ch/pascal.fua">Prof. Pascal Fua</a> and <a href="https://people.epfl.ch/mathieu.salzmann">Dr. Mathieu Salzmann</a>. And I have worked in <a href="http://research.baidu.com/">Baidu Research</a>, where I focus on computer vision and deep learning collaborated with <a href="https://www.uts.edu.au/staff/yi.yang">Prof. Yi Yang</a> in 2019. 
              </p>
			  <p>
                 I did my PhD and Master at <a href="https://www.buaa.edu.cn/">Beihang University</a> (BUAA) in 2019 and 2014, respectively, where I was advised by <a href="http://irip.buaa.edu.cn/yhwang/index.html">Prof. Yunhong Wang</a>. Especially, I had been a visiting PhD at <a href="https://www.rochester.edu/">University of Rochester</a> supervised by <a href="https://www.cs.rochester.edu/u/jluo/">Prof. Jiebo Luo</a> from 2017 to 2018. I did my bachelors at <a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications</a> (BUPT) in 2012.
              </p>
              <p style="text-align:center">
                <a href="mailto:qms@bupt.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/mengshiqi_cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=_gH7-4wAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/mengshi-qi-684abb97/">LinkedIn</a> &nbsp/&nbsp
				<a href="https://dblp.org/pid/191/2586.html">DBLP</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/qimengshi_circle.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/qimengshi_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Interests</heading>
              <p>
                I'm interested in computer vision and machine learning, especially scene understanding, 3D reconstruction and multimedia analysis. Most of my research are about how to understand the semantic content and infer the physical information from images and videos.
              </p>
            </td>
          </tr>
        </tbody></table>
		
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
			  <p>
             [2025.9.18] One paper is accepted by IEEE TIP.
              </p>
			  <p>
             [2025.7.5] One paper is accepted by ACM Multimedia 2025.
              </p>
			  <p>
             [2025.4.21] We release a new VLM-guided Few-shot Video Action Localization in <a href="https://arxiv.org/abs/2504.13460">VAL-VLM</a>.
              </p>
			  <p>
             [2025.4.19] We release a new Robust Scene Graph Generation method as <a href="https://arxiv.org/abs/2504.12606">Robo-SGG</a>.
              </p>
			  <p>
             [2025.4.17] We release a new in-context segmentation method and benchmark as <a href="https://github.com/zaplm/DC-SAM">DC-SAM</a>.
              </p>
			  <p>
             [2025.2.27] Two papers are accepted by CVPR 2025.
              </p>
			 <p>
             [2024.12.9] Two papers are accepted by AAAI 2025.
              </p>
			  <p>
             [2024.7.1] Two papers are accepted by ECCV 2024.
              </p>
              <p>
             [2024.5.15] Call for papers on IEEE TMM Special Issue on Large Multi-modal Models for Dynamic Visual Scene Understanding, please refer to https://signalprocessingsociety.org/blog/ieee-tmm-special-issue-large-multi-modal-models-dynamic-visual-scene-understanding.
              </p>
            </td>
          </tr>
        </tbody></table>
		

		
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
		
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
		
		
		
				  <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025aqa.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2501.03674">
                <papertitle>Action Quality Assessment via Hierarchical Pose-guided Multi-stage Contrastive Regression</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi*</strong>,
			  <a href="">Hao Ye</a>,
			  <a href="">Jiaxuan Peng</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>IIEEE Transactions on Image Processing (TIP)</em>, 2025 
              <br>
              <a href="https://arxiv.org/pdf/2501.03674">pdf</a> /
			  <a href="https://arxiv.org/abs/2501.03674">arxiv</a> /
			  <a href="https://github.com/Lumos0507/HP-MCoRe">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we propose a novel action quality assessment method through hierarchically pose-guided multi-stage contrastive regression. </p>
            </td>
          </tr>
		
		
		
		
																    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025safedriverag.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2507.21585">
                <papertitle>SafeDriveRAG: Towards Safe Autonomous Driving with Knowledge Graph-based Retrieval-Augmented Generation</papertitle>
              </a>
              <br>
			  <a href="">Hao Ye</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Zhaohong Liu</a>,
			  <a href="">Liang Liu</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>ACM MM</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2507.21585">pdf</a> /
			  <a href="https://arxiv.org/abs/2507.21585">arxiv</a> /
			  <a href="https://github.com/Lumos0507/SafeDriveRAG">code</a> /
			  <a href="https://github.com/Lumos0507/SafeDriveRAG">data</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper we instantiate benchmark (SafeDrive228K) and propose a VLM-based baseline with knowledge graph-based retrieval-augmented generation (SafeDriveRAG) for visual question answering (VQA). </p>
            </td>
          </tr>
		
		
														    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025scenegener.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2503.18476">
                <papertitle>Global-Local Tree Search in VLMs for 3D Indoor Scene Generation</papertitle>
              </a>
              <br>
			  <a href="">Wei Deng</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>CVPR</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2503.18476">pdf</a> /
			  <a href="https://arxiv.org/abs/2503.18476">arxiv</a> /
			  <a href="https://github.com/dw-dengwei/TreeSearchGen">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>This paper considers 3D indoor scene generation as a planning problem subject to spatial and layout common sense constraints. To solve the problem with a VLM, we propose a new global-local tree search algorithm. </p>
            </td>
          </tr>
		
		
		
		
												    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2024ttsg.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2411.18894">
                <papertitle>T2SG: Traffic Topology Scene Graph for Topology Reasoning in Autonomous Driving</papertitle>
              </a>
              <br>
			  <a href="">Changsheng Lv</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Liang Liu</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>CVPR</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2411.18894.pdf">pdf</a> /
			  <a href="https://arxiv.org/abs/2411.18894">arxiv</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we defined a novel Traffic Topology Scene Graph, a unified scene graph explicitly modeling the lane, controlled and guided by different road signals (e.g., right turn), and topology relationships among them, which is always ignored by previous high-definition (HD) mapping methods. </p>
            </td>
          </tr>
		
		
				    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2023viot.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.00401">
                <papertitle>VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things</papertitle>
              </a>
              <br>
			  <a href="https://zhongyy.github.io/">Yaoyao Zhong</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Rui Wang</a>,
			  <a href="">Yuhan Qiu</a>,
			  <a href="">Yang Zhang</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>AAAI</em>, 2025 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2312.00401.pdf">pdf</a> /
			  <a href="https://arxiv.org/abs/2312.00401">arxiv</a> /
			  <a href="https://github.com/MICLAB-BUPT/VIoTGPT">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, to address the challenges posed by the fine-grained and interrelated vision tool usage of VIoT, we build VIoTGPT, the framework based on LLMs to correctly interact with humans, query knowledge videos, and invoke vision models to accomplish complicated tasks. </p>
            </td>
          </tr>
		  
		  
		    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2024effi.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2401.02041">
                <papertitle>Efficient Cloud-edge Collaborative Inference for Object Re-identification</papertitle>
              </a>
              <br>
			  <a href="">Chuanming Wang</a>,
			  <a href="">Yuxin Yang</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Huanhuan Zhang</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>AAAI</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2401.02041.pdf">pdf</a> /
			  <a href="https://arxiv.org/abs/2401.02041">arxiv</a> /
			  <a href="https://github.com/MICLAB-BUPT/AAAI2025-DaCM">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we pioneer a cloud-edge collaborative inference framework for ReID systems and particularly propose a distribution-aware correlation modeling network (DaCM) to make the desired image return to the cloud server as soon as possible via learning to model the spatial-temporal correlations among instances. </p>
            </td>
          </tr>
		  
		
				  		 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2024semiaqa.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2407.19675">
                <papertitle>Semi-Supervised Teacher-Reference-Student Architecture for Action Quality Assessment</papertitle>
              </a>
              <br>
			  <a href="">Wulian Yun</a>,
			  <strong>Mengshi Qi*</strong>,
			  <a href="">Fei Peng</a>,
              <a href="">Huadong Ma</a>
              <br>
              <em>ECCV</em>, 2024 
              <br>
			  <a href="https://arxiv.org/abs/2407.19675">pdf</a> /
			  <a href="https://arxiv.org/abs/2303.12332">press</a> /
			  <a href="https://arxiv.org/abs/2303.12332">video</a> /
			  <a href="https://github.com/MICLAB-BUPT/TRS">code</a> /
			  <a href="https://arxiv.org/abs/2407.19675">arxiv</a> /
              <a href="data/qi2021story.bib">bibtex</a>
              <p></p>
              <p>In this paper, we propose a novel semi-supervised method, which can be utilized for better assessment of the AQA task by exploiting a large amount of unlabeled data and a small portion of labeled data.</p>
            </td>
          </tr>	
		  
		  		  		 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2024grasp.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2407.14062">
                <papertitle>Decomposed Vector-Quantized Variational Autoencoder for Human Grasp Generation</papertitle>
              </a>
              <br>
			  <a href="">Zhe Zhao</a>,
			  <strong>Mengshi Qi*</strong>,
              <a href="">Huadong Ma</a>
              <br>
              <em>ECCV</em>, 2024 
              <br>
			  <a href="https://arxiv.org/abs/2407.14062">pdf</a> /
			  <a href="https://arxiv.org/abs/2303.12332">press</a> /
			  <a href="https://arxiv.org/abs/2303.12332">video</a> /
			  <a href="https://github.com/MICLAB-BUPT/D-VQVAE">code</a> /
			  <a href="https://arxiv.org/abs/2407.14062">arxiv</a> /
              <a href="data/qi2021story.bib">bibtex</a>
              <p></p>
              <p>In this paper, we propose a novel Decomposed Vector-Quantized Variational Autoencoder~(DVQ-VAE) to address this limitation by decomposing hand into several distinct parts and encoding them separately.</p>
            </td>
          </tr>	
		  
		
				 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2024sgformer.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2303.11048">
                <papertitle>SGFormer: Semantic Graph Transformer for Point Cloud-based 3D Scene Graph Generation</papertitle>
              </a>
              <br>
			  <a href="">Changsheng Lv</a>,
			  <strong>Mengshi Qi*</strong>,
			  <a href="https://xialipku.github.io/">Xia Li</a>,
              <a href="https://zyang-ur.github.io/">Zhengyuan Yang</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>AAAI</em>, 2024 
              <br>
			  <a href="https://arxiv.org/abs/2303.11048">pdf</a> /
			  <a href="https://arxiv.org/abs/2303.11048">press</a> /
			  <a href="https://arxiv.org/abs/2303.11048">video</a> /
			  <a href="https://github.com/MICLAB-BUPT/SGFormer">code</a> /
			  <a href="https://arxiv.org/abs/2303.11048">arxiv</a> /
              <a href="data/qi2021story.bib">bibtex</a>
              <p></p>
              <p>In this paper, we propose the semantic graph Transformer (SGT) for 3D scene graph generation. The task aims to parse a cloud point-based scene into a semantic structural graph, with the core challenge of modeling the complex global structure. </p>
            </td>
          </tr>	
		  
		  		 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2024weakly.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2303.12332">
                <papertitle>Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature</papertitle>
              </a>
              <br>
			  <a href="">Wulian Yun</a>,
			  <strong>Mengshi Qi*</strong>,
			  <a href="">Chuanming Wang</a>,
              <a href="">Huadong Ma</a>
              <br>
              <em>AAAI</em>, 2024 
              <br>
			  <a href="https://arxiv.org/abs/2303.12332">pdf</a> /
			  <a href="https://arxiv.org/abs/2303.12332">press</a> /
			  <a href="https://arxiv.org/abs/2303.12332">video</a> /
			  <a href="https://github.com/MICLAB-BUPT/ISSF">code</a> /
			  <a href="https://arxiv.org/abs/2303.12332">arxiv</a> /
              <a href="data/qi2021story.bib">bibtex</a>
              <p></p>
              <p> In this paper, we propose a novel weakly supervised temporal action localization method by inferring salient snippet-feature. </p>
            </td>
          </tr>	
		  
		  
		  
		  								    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2024tpami.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2306.03584">
                <papertitle>RDFC-GAN: RGB-Depth Fusion CycleGAN for Indoor Depth Completion</papertitle> 
              </a>
              <br>
			  <a href="">Haowen Wang</a>,
			  <a href="">Zhengping Che</a>,
			  <a href="">Mingyuan Wang</a>,
			  <a href="">Zhiyuan Xu</a>,
			  <a href="">Xiuquan Qiao</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Feifei Feng</a>,
			  <a href="">Jian Tang</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2024 &nbsp <font color="red"><strong>(An extension of our CVPR 22' paper)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2306.03584">pdf</a> /
			  <a href="https://arxiv.org/abs/2306.03584">arxiv</a> /
			  <a href="">press</a> /
			  <a href="">code</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we design a novel two-branch end-to-end fusion network named RDFC-GAN, which takes a pair of RGB and incomplete depth images as input to predict a dense and completed depth map.</p>
            </td>
          </tr>
		  
		  
		
								    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2024mutual.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2401.06430">
                <papertitle>Mutual Distillation Learning For Person Re-Identification</papertitle>
              </a>
              <br>
			  <a href="">Huiyuan Fu</a>,
			  <a href="">Kuilong Cui</a>,
			  <a href="">Chuanming Wang</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>IEEE Transactions on Multimedia (TMM)</em>, 2024
              <br>
              <a href="https://ieeexplore.ieee.org/document/10493202">pdf</a> /
			  <a href="https://arxiv.org/abs/2401.06430.pdf">arxiv</a> /
			  <a href="https://ieeexplore.ieee.org/document/10493202">press</a> /
			  <a href="https://github.com/KuilongCui/MDPR">code</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we propose a novel approach, Mutual Distillation Learning For Person Re-identification (termed as MDPR), which addresses the challenging problem from multiple perspectives within a single unified model, leveraging the power of mutual distillation to enhance the feature representations collectively.</p>
            </td>
          </tr>
		  
		  
		  		 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2023disentangle.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2310.19559.pdf">
                <papertitle>Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning</papertitle>
              </a>
              <br>
			  <a href="">Changsheng Lv</a>,
			  <a href="">Shuai Zhang</a>,
			  <a href="https://www.yapengtian.com/">Yapeng Tian</a>,
			  <strong>Mengshi Qi*</strong>,
              <a href="">Huadong Ma</a>
              <br>
              <em>NeurIPS</em>, 2023 
              <br>
			  <a href="https://arxiv.org/pdf/2310.19559.pdf">pdf</a> /
			  <a href="https://arxiv.org/pdf/2310.19559.pdf">press</a> /
			  <a href="https://arxiv.org/pdf/2310.19559.pdf">video</a> /
			  <a href="https://arxiv.org/abs/2310.19559">arxiv</a> /
			  <a href="https://github.com/Andy20178/DCL">code</a> /
              <a href="data/qi2021story.bib">bibtex</a>
              <p></p>
              <p>In this paper, we propose a Disentangled Counterfactual Learning~(DCL) approach for physical audiovisual commonsense reasoning. The task aims to infer objects' physics commonsense based on both video and audio input, with the main challenge is how to imitate the reasoning ability of humans.  </p>
            </td>
          </tr>	
		  
		  
		  		 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2023unsupervised.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Unsupervised_Self-Driving_Attention_Prediction_via_Uncertainty_Mining_and_Knowledge_Embedding_ICCV_2023_paper.pdf">
                <papertitle>Unsupervised Self-Driving Attention Prediction via Uncertainty Mining and Knowledge Embedding</papertitle>
              </a>
              <br>
			  <a href="">Pengfei Zhu</a>,
			  <strong>Mengshi Qi*</strong>,
			  <a href="https://xialipku.github.io/">Xia Li</a>,
			  <a href="https://weijian-li.github.io/">Weijian Li</a>,
              <a href="">Huadong Ma</a>
              <br>
              <em>ICCV</em>, 2023 
              <br>
			  <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Unsupervised_Self-Driving_Attention_Prediction_via_Uncertainty_Mining_and_Knowledge_Embedding_ICCV_2023_paper.pdf">pdf</a> /
			  <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Unsupervised_Self-Driving_Attention_Prediction_via_Uncertainty_Mining_and_Knowledge_Embedding_ICCV_2023_paper.html">press</a> /
			  <a href="https://arxiv.org/abs/2303.09706">video</a> /
			  <a href="https://arxiv.org/abs/2303.09706">arxiv</a> /
			  <a href="https://github.com/zaplm/DriverAttention">code</a> /
              <a href="data/qi2021story.bib">bibtex</a>
              <p></p>
              <p>In this paper, we are the first to introduce an unsupervised way to predict self-driving attention by uncertainty modeling and driving knowledge integration. </p>
            </td>
          </tr>	
		  
		  
		  	
		<tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2023gait.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10025772">
                <papertitle>GaitReload: A Reloading Framework for Defending Against On-Manifold Adversarial Gait Sequences</papertitle>
              </a>
              <br>
			  <a href="">Peilun Du</a>,
			  <a href="https://xiaolongbupt.github.io/">Xiaolong Zheng</a>,
			  <strong>Mengshi Qi</strong>,
              <a href="">Huadong Ma</a>
              <br>
              <em>IEEE Transactions on Information Forensics and Security (TIFS)</em>, 2023
              <br>
			  <a href="https://ieeexplore.ieee.org/document/10025772">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/document/10025772">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p> In this paper, we propose GaitReload, a post-processing adversarial defense method to defend against AWP for the gait recognition model with sequenced inputs.</p>
            </td>
          </tr>
		  
		  		<tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2022fusiongan.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9878677">
                <papertitle>RGB-Depth Fusion GAN for Indoor Depth Completion</papertitle>
              </a>
              <br>
			  <a href="">Haowen Wang</a>,
			  <a href="">Mingyuan Wang</a>,
			  <a href="">Zhengping Che</a>,
			  <a href="">Zhiyuan Xu</a>,
			  <a href="">Xiuquan Qiao</a>,
			  <strong>Mengshi Qi</strong>,
              <a href="">Feifei Feng</a>,
			  <a href="">Jian Tang</a>
              <br>
              <em>CVPR </em>, 2022
              <br>
			  <a href="https://ieeexplore.ieee.org/document/9878677">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/document/9878677">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p> In this paper, we design a novel two-branch end-to-end fusion network, which takes a pair of RGB and incomplete depth images as input to predict a dense and completed depth map.</p>
            </td>
          </tr>
		  

		
	
		<tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2020seman.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9351755">
                <papertitle>Semantics-Aware Spatial-Temporal Binaries for Cross-Modal Video Retrieval</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
			  <a href="https://sites.google.com/site/firmamentqj/">Jie Qin</a>,
			  <a href="https://www.uts.edu.au/staff/yi.yang">Yi Yang</a>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>IEEE Transactions on Image Processing (TIP)</em>, 2021
              <br>
			  <a href="https://ieeexplore.ieee.org/document/9351755">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/document/9351755">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we propose a novel binary representation learning framework, named Semantics-aware Spatial-temporal Binaries (S<sup>2</sup>Bin), which simultaneously considers spatial-temporal context and semantic relationships for cross-modal video retrieval. By exploiting the semantic relationships between two modalities, S<sup>2</sup>Bin can efficiently and effectively generate binary codes for both videos and texts. In addition, we adopt an iterative optimization scheme to learn deep encoding functions with attribute-guided stochastic training.</p>
            </td>
          </tr>
		  
		 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2021story.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2021story.pdf">
                <papertitle>Latent Memory-augmented Graph Transformer for Visual Storytelling</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
			  <a href="https://sites.google.com/site/firmamentqj/">Jie Qin</a>,
              <a href="http://irip.buaa.edu.cn/dihuang/index.html">Di Huang</a>,
			  <a href="http://zhiqiangshen.com/">Zhiqiang Shen</a>,
			  <a href="https://www.uts.edu.au/staff/yi.yang">Yi Yang</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>ACM MM</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
			  <a href="data/qi2021story.pdf">pdf</a> /
			  <a href="https://dl.acm.org/doi/10.1145/3474085.3475236">press</a> /
			  <a href="https://dl.acm.org/doi/10.1145/3474085.3475236">video</a> /
              <a href="data/qi2021story.bib">bibtex</a>
              <p></p>
              <p>In this paper, we present a novel Latent Memory-augmented Graph Transformer (LMGT), a Transformer based framework including a designed graph encoding module and a latent memory unit for visual story generation. </p>
            </td>
          </tr>	

	
		 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2020imi.jpg' width="180" height="160">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_Imitative_Non-Autoregressive_Modeling_for_Trajectory_Forecasting_and_Imputation_CVPR_2020_paper.pdf">
                <papertitle>Imitative Non-Autoregressive Modeling for Trajectory Forecasting and Imputation</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
              <a href="https://sites.google.com/site/firmamentqj/">Jie Qin</a>,
              <a href="https://yu-wu.net/">Yu Wu</a>,
			  <a href="https://www.uts.edu.au/staff/yi.yang">Yi Yang</a>
              <br>
              <em>CVPR</em>, 2020
              <br>
              <a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Qi_Imitative_Non-Autoregressive_Modeling_for_Trajectory_Forecasting_and_Imputation_CVPR_2020_paper.pdf">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/document/9156400">press</a> /
              <a href="https://www.youtube.com/watch?v=0eiXmWGDJNs">video</a> /
			  <a href="data/qi2020imi.bib">bibtex</a>
              <p></p>
              <p> In this paper, we proposed a novel Imitative Non-Autoregressive Modeling method to bridge the performance gap between autoregressive and non-autoregressive models for temporal sequence forecasting and imputation. Our proposed framework leveraged an imitation learning fashion including two parts, i.e., a recurrent conditional variational autoencoder (RC-VAE) demonstrator and a nonautoregressive transformation model (NART) learner.</p>
            </td>
          </tr>

		 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2020fewshot.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2020few.pdf">
                <papertitle>Few-Shot Ensemble Learning for Video Classificaion with SlowFast Memory Networks</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
			  <a href="https://sites.google.com/site/firmamentqj/">Jie Qin</a>,
              <a href="https://scholar.google.ca/citations?user=DnBb3e0AAAAJ&hl=en">Xiantong Zhen</a>,
              <a href="http://irip.buaa.edu.cn/dihuang/index.html">Di Huang</a>,
			  <a href="https://www.uts.edu.au/staff/yi.yang">Yi Yang</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>ACM MM</em>, 2020
              <br>
			  <a href="data/qi2020few.pdf">pdf</a> /
			  <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3416269">press</a> /
			  <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3416269">video</a> /
              <a href="data/qi2020fewshot.bib">bibtex</a>
              <p></p>
              <p>In this paper, we address few-shot video classification by learning an ensemble of SlowFast networks augmented with memory units. Specifically, we introduce a family of few-shot learners based on SlowFast networks which are used to extract informative features at multiple rates, and we incorporate a memory unit into each network to enable encoding and retrieving crucial information instantly.</p>
            </td>
          </tr>		  
		
		 <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2020stc.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2020stcgan.pdf">
                <papertitle>STC-GAN: Spatio-Temporally Coupled Generative Adversarial Networks for Predictive Scene Parsing</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>IEEE Transactions on Image Processing (TIP)</em>, 2020
              <br>
			  <a href="data/qi2020stcgan.pdf">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/document/9052709">press</a> /
              <a href="data/qi2020stc.bib">bibtex</a>
              <p></p>
              <p>In this paper, we present a novel Generative Adversarial Networks-based model (i.e., STC-GAN) for predictive scene parsing. STC-GAN captures both spatial and temporal representations from the observed frames of a video through CNN and convolutional LSTM network. Moreover, a coupled architecture is employed to guide the adversarial training via a weight-sharing mechanism and a feature adaptation transform between the future frame generation model and the predictive scene parsing model.</p>
            </td>
          </tr>
		  
		  
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2019sports.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function nightsight_start() {
                  document.getElementById('nightsight_image').style.opacity = "1";
                }

                function nightsight_stop() {
                  document.getElementById('nightsight_image').style.opacity = "0";
                }
                nightsight_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2019sports.pdf">
                <papertitle>Sports Video Captioning via Attentive Motion Representation and Group Relationship Modeling</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi</strong>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>IEEE Transactions on Circuits and Systems fo Video Technology (TCSVT)</em>, 2019 &nbsp <font color="red"><strong>(An extension of our MMSports@MM 18' paper)</strong></font>
              <br>
              <a href="data/qi2019sports.pdf">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/abstract/document/8733019">press</a> /
              <a href="data/qi2019sports.bib">bibtex</a>
              <p></p>
              <p>  In this study, we present a novel hierarchical recurrent neural network based framework with an attention mechanism for sports video captioning, in which a motion representation module is proposed to capture individual pose attribute and dynamical trajectory cluster information with extra professional sports knowledge, and a group relationship module is employed to design a scene graph for modeling players’ interaction by a gated graph convolutional network.</p>
            </td>
          </tr>
          
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2019stagnet.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function font_start() {
                  document.getElementById('font_image').style.opacity = "1";
                }

                function font_stop() {
                  document.getElementById('font_image').style.opacity = "0";
                }
                font_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2019stagnet.pdf">
                <papertitle>stagNet: An Attentive Semantic RNN for Group Activity and Individual Action Recognition</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="https://sites.google.com/site/firmamentqj/">Jie Qin</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>,
			  <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>,
			  <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a>
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2019 &nbsp <font color="red"><strong>(An extension of our ECCV 18' paper)</strong></font>
              <br>
              <a href="data/qi2019stagnet.pdf">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/abstract/document/8621027">press</a> /
              <a href="data/qi2019stagnet.bib">bibtex</a>
              <p></p>
              <p> In the paper, we present a novel attentive semantic recurrent neural network (RNN), namely stagNet, for understanding group activities and individual actions in videos, by combining the spatio-temporal attention mechanism and semantic graph modeling. Specifically, a structured semantic graph is explicitly modeled to express the spatial contextual content of the whole scene, which is afterward further incorporated with the temporal factor through structural- RNN. </p>
            </td>
          </tr>
          
          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2019attentive.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function dpzlearn_start() {
                  document.getElementById('dpzlearn_image').style.opacity = "1";
                }

                function dpzlearn_stop() {
                  document.getElementById('dpzlearn_image').style.opacity = "0";
                }
                dpzlearn_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qi_Attentive_Relational_Networks_for_Mapping_Images_to_Scene_Graphs_CVPR_2019_paper.pdf">
                <papertitle>Attentive Relational Networks for Mapping Images to Scene Graphs</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi*</strong>,
              <a href="https://www.cs.rochester.edu/u/wli69/">Weijian Li*</a>,
              <a href="https://www.cs.rochester.edu/u/zyang39/">Zhengyuan Yang</a>,
			  <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
			  <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>CVPR</em>, 2019
              <br>
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qi_Attentive_Relational_Networks_for_Mapping_Images_to_Scene_Graphs_CVPR_2019_paper.pdf">pdf</a> /
			  <a href="https://arxiv.org/abs/1811.10696">arxiv</a> /
			  <a href="https://ieeexplore.ieee.org/document/8954105">press</a> /
              <a href="data/qi2019attentive.bib">bibtex</a>
              <p></p>
              <p> In this study, we propose a novel Attentive Relational Network that consists of two key modules with an object detection backbone to approach this problem. The first module is a semantic transformation module utilized to capture semantic embedded relation features, by translating visual features and linguistic features into a common semantic space. The other module is a graph self-attention module introduced to embed a joint graph representation through assigning various importance weights to neighboring nodes. </p>
            </td>
          </tr>
          
          <tr onmouseout="porlight_stop()" onmouseover="porlight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2019ke.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function porlight_start() {
                  document.getElementById('porlight_image').style.opacity = "1";
                }

                function porlight_stop() {
                  document.getElementById('porlight_image').style.opacity = "0";
                }
                porlight_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qi_KE-GAN_Knowledge_Embedded_Generative_Adversarial_Networks_for_Semi-Supervised_Scene_Parsing_CVPR_2019_paper.pdf">
                <papertitle>KE-GAN: Knowledge Embedded Generative Adversarial Networks for Semi-Supervised Scene Parsing</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="https://sites.google.com/site/firmamentqj/">Jie Qin</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>
              <br>
              <em>CVPR</em>, 2019
              <br>
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Qi_KE-GAN_Knowledge_Embedded_Generative_Adversarial_Networks_for_Semi-Supervised_Scene_Parsing_CVPR_2019_paper.pdf">pdf</a> /
              <a href="https://ieeexplore.ieee.org/document/8953263">press</a> /
			  <a href="data/qi2019ke.bib">bibtex</a>
              <br>
              <p></p>
              <p>In this paper, we propose a novel Knowledge Embedded Generative Adversarial Networks, dubbed as KE-GAN, to tackle the challenging problem in a semi-supervised fashion. KE-GAN captures semantic consistencies of different categories by devising a Knowledge Graph from the large-scale text corpus.</p>
            </td>
          </tr>

          <tr onmouseout="mpi_stop()" onmouseover="mpi_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2018stagnet.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function loss_start() {
                  document.getElementById('loss_image').style.opacity = "1";
                }

                function loss_stop() {
                  document.getElementById('loss_image').style.opacity = "0";
                }
                loss_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Mengshi_Qi_stagNet_An_Attentive_ECCV_2018_paper.pdf">
                <papertitle>stagNet: An Attentive Semantic RNN for Group Activity Recognition</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi</strong>,
              <a href="https://sites.google.com/site/firmamentqj/">Jie Qin</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
			  <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>,
			  <a href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjQ4LC0xOTcxNDY1MTc4.html">Luc Van Gool</a>
              <br>
              <em>ECCV</em>, 2018
              <br>
              <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Mengshi_Qi_stagNet_An_Attentive_ECCV_2018_paper.pdf">pdf</a> /
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-01249-6_7">press</a> /
			  <a href="data/qi2018stagnet.bib">bibtex</a>
              <p></p>
              <p> We propose a novel attentive semantic recurrent neural network (RNN), dubbed as stagNet, for understanding group activities in videos, based on the spatio-temporal attention and semantic graph.  </p>
            </td>
          </tr>

          <tr onmouseout="mpi_stop()" onmouseover="mpi_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2018sports.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function mpi_start() {
                  document.getElementById('mpi_image').style.opacity = "1";
                }

                function mpi_stop() {
                  document.getElementById('mpi_image').style.opacity = "0";
                }
                mpi_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2018sports.pdf">
                <papertitle>Sports Video Captioning by Attentive Motion Representation based Hierarchical Recurrent Neural Networks</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi</strong>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>,
              <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a>
              <br>
              <em>MMSports@MM</em>, 2018 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="data/qi2018sports.pdf">pdf</a> /
			  <a href="https://dl.acm.org/citation.cfm?id=3265851">press</a> /
              <a href="https://www.youtube.com/watch?v=AUHGMF-kvDc">video</a> /		  
              <a href="data/qi2018sports.bib">bibtex</a>
              <p></p>
              <p> In this paper, we present a novel hierarchical recurrent neural network (RNN) based framework with an attention mechanism for sports video captioning. A motion representation module is proposed to extract individual pose attribute and group-level trajectory cluster information. </p>
            </td>
          </tr>

          <tr onmouseout="unprocessing_stop()" onmouseover="unprocessing_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2017online.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function unprocessing_start() {
                  document.getElementById('unprocessing_image').style.opacity = "1";
                }

                function unprocessing_stop() {
                  document.getElementById('unprocessing_image').style.opacity = "0";
                }
                unprocessing_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2017online.pdf">
                <papertitle>Online Cross-modal Scene Retrieval by Binary Representation and Semantic Graph</papertitle>
              </a>
              <br>
			  <strong>Mengshi Qi</strong>,
              <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>,
              <a href="http://irip.buaa.edu.cn/anli/index.html">Annan Li</a>
              <br>
              <em>ACM MM</em>, 2017
              <br>
              <a href="data/qi2017online.pdf">pdf</a> /
              <a href="https://dl.acm.org/citation.cfm?id=3123311">press</a> /
              <a href="data/qi2017online.bib">bibtex</a>
              <p></p>
              <p>We propose a new framework for online cross-modal scene retrieval based on binary representations and semantic graph. Specially, we adopt the cross-modal hashing based on the quantization loss of different modalities. By introducing the semantic graph, we are able to extract wealthy semantics and measure their correlation across different modalities. </p>
            </td>
          </tr>

          <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2016deep.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/qi2016deep.pdf">
                <papertitle>DEEP-CSSR: Scene Classification using Category-specific Salient Region with Deep Features</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi</strong>,
			  <a href="http://irip.buaa.edu.cn/yhwang/index.html">Yunhong Wang</a>
              <br>
              <em>IEEE ICIP</em>, 2016 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="data/qi2016deep.pdf">pdf</a> /
			  <a href="https://ieeexplore.ieee.org/abstract/document/7532517">press</a> /
              <a href="data/qi2016deep.bib">bibtex</a>
              <p></p>
              <p>In this paper, we introduce a novel framework towards scene classification using category-specific salient region(CSSR) with deep CNN features, called Deep-CSSR. </p>
            </td>
          </tr>
		  

		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Pre-Print</heading>
            </td>
          </tr>
        </tbody></table>
		
		  		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
				
				
				
																														    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025dcsam.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2504.12080">
                <papertitle>DC-SAM: In-Context Segment Anything in Images and Videos via Dual Consistency</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi*</strong>,
			  <a href="">Pengfei Zhu</a>,
			  <a href="https://lxtgh.github.io/">Xiangtai Li</a>,
			  <a href="">Xiaoyang Bi</a>,
			  <a href="http://luqi.info/">Lu Qi</a>,
			  <a href="">Huadong Ma</a>,
			  <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
              <br>
              <em>Arxiv</em>, 2025 
              <br>
              <a href="https://arxiv.org/pdf/2504.12080">pdf</a> /
			  <a href="https://arxiv.org/abs/2504.12080">arxiv</a> /
			  <a href="https://github.com/zaplm/DC-SAM">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we propose the Dual Consistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2 for in-context segmentation of both images and videos.. </p>
            </td>
          </tr>
		  
				
				
				
				
																										    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025grasp.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2501.05483">
                <papertitle>Human Grasp Generation for Rigid and Deformable Objects with Decomposed VQ-VAE</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi*</strong>,
			  <a href="">Zhe Zhao</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2025 &nbsp <font color="red"><strong>(An extension of our ECCV 24' paper)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2501.05483">pdf</a> /
			  <a href="https://arxiv.org/abs/2501.05483">arxiv</a> /
			  <a href="https://github.com/florasion/D-VQVAE">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we propose a novel improved Decomposed Vector-Quantized Variational Autoencoder (DVQ-VAE-2) with a new Mesh UFormer as the backbone network to extract the hierarchical structural representations from the mesh and propose a new normal vector-guided position encoding to simulate the hand-object deformation. </p>
            </td>
          </tr>
		  
		  				    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025RDCL.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2502.12425">
                <papertitle>Robust Disentangled Counterfactual Learning for Physical Audiovisual Commonsense Reasoning</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi*</strong>,
			  <a href="">Changsheng Lv</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2025 &nbsp <font color="red"><strong>(An extension of our NeurIPS 23' paper)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2502.12425">pdf</a> /
			  <a href="https://arxiv.org/abs/2502.12425">arxiv</a> /
			  <a href="https://github.com/MICLAB-BUPT/DCL">code</a> /
			  <a href="https://github.com/MICLAB-BUPT/DCL">data</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we propose a new Robust Disentangled Counterfactual Learning (RDCL) approach for physical audiovisual commonsense reasoning. </p>
            </td>
          </tr>
				
				
				    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025driveattention.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2501.15045">
                <papertitle>Towards Robust Unsupervised Attention Prediction in Autonomous Driving</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi*</strong>,
			  <a href="">Xiaoyang Bi</a>,
			  <a href="">Pengfei Zhu</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2025 &nbsp <font color="red"><strong>(An extension of our ICCV 23' paper)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2501.15045">pdf</a> /
			  <a href="https://arxiv.org/abs/2501.15045">arxiv</a> /
			  <a href="https://github.com/zaplm/DriverAttention">code</a> /
			  <a href="https://github.com/zaplm/DriverAttention">data</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we propose a novel robust unsupervised attention prediction method, which includes an Uncertainty Mining Branch,  Knowledge Embedding Block, and RoboMixup, a novel data augmentation method to improve robustness against corruption and central bias. </p>
            </td>
          </tr>
		  
		 	  		  
		  	<tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025fewshot.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2504.13460">
                <papertitle>Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization</papertitle>
              </a>
              <br>
			  <a href="">Hongwei Ji</a>,
			  <a href="">Wulian Yun</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2504.13460">pdf</a> /
			  <a href="https://arxiv.org/abs/2504.13460">arxiv</a> /
			  <a href="">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection.</p>
            </td>
          </tr>
		  
		  
		  
		  
		  		  																		    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025roboformer.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2504.12606">
                <papertitle>Robo-SGG: Exploiting Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation</papertitle>
              </a>
              <br>
			  <a href="">Changsheng Lv</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Zijian Fu</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2504.12606">pdf</a> /
			  <a href="https://arxiv.org/abs/2504.12606">arxiv</a> /
			  <a href="">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we introduce a novel method named Robo-SGG, i.e., Layout-Oriented Normalization and Restitution for Robust Scene Graph Generation. Compared to the existing SGG setting, the robust scene graph generation aims to perform inference on a diverse range of corrupted images, with the core challenge being the domain shift between the clean and corrupted images. </p>
            </td>
          </tr>
		  
		  
		  
		  																									    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025continual.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2502.00843">
                <papertitle>VLM-Assisted Continual learning for Visual Question Answering in Self-Driving</papertitle>
              </a>
              <br>
			  <a href="">Yuxin Lin</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Liang Liu</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2502.00843">pdf</a> /
			  <a href="https://arxiv.org/abs/2502.00843">arxiv</a> /
			  <a href="">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we present a novel continual learning framework that combines VLMs with selective memory replay and knowledge distillation, reinforced by task-specific projection layer regularization.</p>
            </td>
          </tr>
				
				
				
																									    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025missingtra.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2501.16767">
                <papertitle>Target-driven Self-Distillation for Partial Observed Trajectories Forecasting</papertitle>
              </a>
              <br>
			  <a href="">Pengfei Zhu</a>,
			  <a href="">Peng Shu</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Liang Liu</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2501.16767">pdf</a> /
			  <a href="https://arxiv.org/abs/2501.16767">arxiv</a> /
			  <a href="">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we introduce a Target-driven Self-Distillation method (TSD) for motion forecasting. Our method leverages predicted accurate targets to guide the model in making predictions under partial observation condition.</p>
            </td>
          </tr>
				
				
				
				
																						    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025pose.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2501.05264">
                <papertitle>Towards Balanced Continual Multi-Modal Learning in Human Pose Estimation</papertitle>
              </a>
              <br>
			  <a href="">Jiaxuan Peng</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Dong Zhao</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2501.05264">pdf</a> /
			  <a href="https://arxiv.org/abs/2501.05264">arxiv</a> /
			  <a href="">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we introduce a novel balanced continual multi-modal learning method for 3D HPE, which harnesses the power of RGB, LiDAR, mmWave, and WiFi. </p>
            </td>
          </tr>
				
				
				
				

		  
		  
		  																		    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2025humanpose.png' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2501.09565">
                <papertitle>A New Teacher-Reviewer-Student Framework for Semi-supervised 2D Human Pose Estimation</papertitle>
              </a>
              <br>
			  <a href="">Wulian Yun</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Fei Peng</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2025 
              <br>
              <a href="https://arxiv.org/pdf/2501.09565">pdf</a> /
			  <a href="https://arxiv.org/abs/2501.09565">arxiv</a> /
			  <a href="https://github.com/Lumos0507/HP-MCoRe">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we propose a novel semi-supervised 2D human pose estimation method by utilizing a newly designed Teacher-Reviewer-Student framework.</p>
            </td>
          </tr>
				
														    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2024trajectory.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2412.15673">
                <papertitle>Learning Group Interactions and Semantic Intentions for Multi-Object Trajectory Prediction</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi*</strong>,
			  <a href="">Yuxin Yang</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2412.15673.pdf">pdf</a> /
			  <a href="https://arxiv.org/abs/2412.15673">arxiv</a> /
			  <a href="https://github.com/MICLAB-BUPT/Group2Int-trajectory">code</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we propose a novel diffusion-based trajectory prediction framework that integrates group-level interactions into a conditional diffusion model, enabling the generation of diverse trajectories aligned with specific group activity. </p>
            </td>
          </tr>



										    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2024bn.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2411.18860">
                <papertitle>Improving Batch Normalization with TTA for Robust Object Detection in Self-Driving</papertitle>
              </a>
              <br>
			  <a href="">Dacheng Liao</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Liang Liu</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2411.18860.pdf">pdf</a> /
			  <a href="https://arxiv.org/abs/2411.18860">arxiv</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we introduce a LearnableBN layer based on Generalized-search Entropy Minimization (GSEM) method, and a new semantic-consistency based dual-stage-adaptation strategy. </p>
            </td>
          </tr>

				
										    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2024tkde.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2408.08488">
                <papertitle>Adversarial Contrastive Learning Based Physics-Informed Temporal Networks for Cuffless Blood Pressure Estimation</papertitle>
              </a>
              <br>
			  <a href="">Rui Wang</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Yingxia Shao</a>,
			  <a href="">Anfu Zhou</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2408.08488.pdf">pdf</a> /
			  <a href="https://arxiv.org/abs/2408.08488">arxiv</a> /
              <a href="https://github.com/MICLAB-BUPT/ACL-PITN">code</a> /			  
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we introduce a novel physics-informed temporal network~(PITN) with adversarial contrastive learning to enable precise Blood Pressure estimation with very limited data. </p>
            </td>
          </tr>

						  
		  
		  				    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2024uncover.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2401.02916">
                <papertitle>Uncovering the human motion pattern: Pattern Memory-based Diffusion Model for Trajectory Prediction</papertitle>
              </a>
              <br>
			  <a href="">Yuxin Yang</a>,
			  <a href="">Pengfei Zhu</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2401.02916.pdf">pdf</a> /
			  <a href="https://arxiv.org/abs/2401.02916">arxiv</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we introduce a novel memory-based method, named Motion Pattern Priors Memory Network. Our method involves constructing a memory bank derived from clustered prior knowledge of motion patterns observed in the training set trajectories.  </p>
            </td>
          </tr>
		
		
		  
		  		    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/qi2022denoise.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2205.00214">
                <papertitle>Coarse-to-Fine Video Denoising with Dual-Stage Spatial-Channel Transformer</papertitle>
              </a>
              <br>
			  <a href="">Wulian Yun</a>,
              <strong>Mengshi Qi*</strong>,
			  <a href="">Chuanming Wang</a>,
			  <a href="">Huiyuan Fu</a>,
			  <a href="">Huadong Ma</a>
              <br>
              <em>Arxiv</em>, 2022
              <br>
              <a href="https://arxiv.org/pdf/2205.00214.pdf">pdf</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p> In this paper, we propose a Dual-stage Spatial-Channel Transformer for coarse-to-fine video denoising, which inherits the advantages of both Transformer and CNNs. </p>
            </td>
          </tr>
		
		
		    <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/hand_pose.jpg' width="180" height="180">
              </div>
              <script type="text/javascript">
                function motionblur_start() {
                  document.getElementById('motionblur_image').style.opacity = "1";
                }呢

                function motionblur_stop() {
                  document.getElementById('motionblur_image').style.opacity = "0";
                }
                motionblur_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2012.11260">
                <papertitle>Unsupervised Domain Adaptation with Temporal-Consistent Self-Training for 3D Hand-Object Joint Reconstruction</papertitle>
              </a>
              <br>
              <strong>Mengshi Qi</strong>,
			  <a href="https://people.epfl.ch/edoardo.remelli">Edoardo Remelli</a>,
			  <a href="https://people.epfl.ch/mathieu.salzmann">Mathieu Salzmann</a>,
			  <a href="https://people.epfl.ch/pascal.fua">Pascal Fua</a>
              <br>
              <em>Arxiv</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2012.11260.pdf">pdf</a> /
			  <a href="">press</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>In this paper, we introduce an effective approach to exploit 3D geometric constraints within a cycle generative adversarial network (CycleGAN) to perform domain adaptation. Furthermore, we propose to enforce short- and long-term temporal consistency to fine-tune the domain-adapted model in a self-supervised fashion. </p>
            </td>
          </tr>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              Conference Reviewer, ICCV 2019-2023, CVPR 2020-2025, ECCV 2020-2022, ICML 2021-2025, ICLR 2021-2025, NeurIPS 2020-2023, ACM MM 2021-2023
              <br>
			  <br>
              Journal Reviewer, TPAMI, IJCV, TIP, TMM, TCSVT, PR, ACM Computing Surveys
			  <br>
			  <br>
              Guest Editor, IEEE TMM Special Issue on Large Multi-modal Models for Dynamic Visual Scene Understanding
			  <br>
			  <br>
              Senior PC Member, IJCAI 2021/2023-2025, AAAI 2023-2025
			  <br>
              <br>
              PC Member, AAAI 2020-2022, IJCAI 2020/2022
			  <br>
              <br>
              Area Chair, ICME 2024/2025
			  <br>
              <br>
              IEEE Member, ACM Member, CCF Member, CAAI Member and CSIG Member
            </td>
          </tr>
        </tbody></table>
		
		        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Students</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/students.png"></td>
            <td width="75%" valign="center">
              Wulian Yun (PhD student, 2020-), co-supervised with Prof. Huadong Ma (1 CCF-A paper, 1 CCF-B paper, 2 patents).
			  <br>
			  <br>
              Changsheng Lv (PhD student, 2022-) (3 CCF-A papers, 1 patent).
			  <br>
			  <br>
              <a href="https://zest86.github.io/">Rui Wang</a> (PhD student, 2023-) (1 CCF-A paper).
			  <br>
			  <br>
			  Yonghao Zhou (PhD student, 2023-), co-supervised with Prof. Huadong Ma.
			  <br>
			  <br>
              Wei Deng (PhD student, 2024-) (1 CCF-A paper).
			  <br>
			  <br>
              Dacheng Liao (PhD student, 2024-), co-supervised with Prof. Liang Liu (1 patent).
			  <br>
			  <br>
              Shengzhe Xuecao (PhD student, 2025-).
			  <br>
			  <br>
              Yuxin Yang (Master student, 2023-) (1 CCF-A paper, 1 CCF-C paper, 1 patent).
			  <br>
			  <br>
              Zhe Zhao (Master student, 2023-) (1 CCF-B paper, 1 patent).
			  <br>
			  <br>
              Jiaxuan Peng (Master student, 2023-) (1 patent).
			  <br>
			  <br>
              Yuxin Lin (Master student, 2023-), co-supervised with Prof. Liang Liu (1 patent).
			  <br>
			  <br>
              Xiaoyang Bi (Master student, 2024-).
			  <br>
			  <br>
              Hao Ye (Master student, 2024-)(1 CCF-A paper, 1 patent).
			  <br>
			  <br>
              Zijian Fu (Master student, 2024-)(1 patent).
			  <br>
			  <br>
              Hongwei Ji (Master student, 2024-)(1 patent).
			  <br>
			  <br>
              Yeteng Wu (Master student, 2024-).
			  <br>
			  <br>
              Bo Gao (Master student, 2024-), co-supervised with Prof. Huanhuan Zhang.
			  <br>
			  <br>
              Peng Shu (Master student, 2024-), co-supervised with Prof. Liang Liu (1 patent).
			  <br>
			  <br>
              Zhining Zhang (Master student, 2024-), co-supervised with Prof. Liang Liu.
			  <br>
			  <br>
              Shuaikun Liu (Master student, 2025-).
			  <br>
			  <br>
              Zhaohong Liu (Master student, 2025-)(1 CCF-A paper).
			  <br>
			  <br>
              Dongqing Liu (Master student, 2025-).
			  <br>
			  <br>
              Yilin Ou (Master student, 2025-).
			  <br>
			  <br>
              Shuo Wang (Master student, 2026-).
            </td>
          </tr>
        </tbody></table>
		
		 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Alumni</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/alumns.jpg"></td>
            <td width="75%" valign="center">
              Qi An (Master student, 2021-2024), co-supervised with Prof. Huadong Ma (1 CCF-B paper, 1 CCF-A workshop paper), working at POSTAL SAVINGS BANK OF CHINA.
			  <br>
			  <br>
              Rongshuai Liu (Master student, 2021-2024), co-supervised with Prof. Huadong Ma (1 CCF-A workshop paper, 1 patent), working at ByteDance.
			  <br>
			  <br>
              Kuilong Cui (Master student, 2021-2024), co-supervised with Prof. Huiyuan Fu (1 CCF-B Transactions paper), working at Alibaba.
			  <br>
			  <br>
              Pengfei Zhu (Master student, 2022-2025) (1 CCF-A paper, 1 CCF-C paper, 1 patent), working at Tencent.
			  <br>
			  <br>
              Shuai Zhang (Master student, 2022-2025) (1 CCF-A paper, 1 patent), working at Huawei.
			  <br>
			  <br>
              Yanshu He (Master student, 2022-2025), co-supervised with Prof. Huadong Ma, working at China Construction Bank.
			  <br>
			  <br>
              Yuang Liu (Master student, 2022-2025), co-supervised with Prof. Liang Liu (1 CCF-C paper, 1 patent), working at Baidu.
            </td>
          </tr>
        </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <br>
              <br>
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks to this <a href="https://jonbarron.info/">awesome guy</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
		
		
      </td>
    </tr>
  </table>
</body>

</html>
